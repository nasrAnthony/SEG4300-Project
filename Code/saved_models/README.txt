increasing MLP hidden dim to 128 and adding another layer
2 lstm layers, 0.2 dropout, leakrelu for mlp
in file


increasing total model hidden dim to 256, added lr scheduler and lr = 0.001
in file

batch size 128 and dropout 0.3 (cumulative with above)
