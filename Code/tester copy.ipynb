{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.multihead_pitch_prediction' from 'c:\\\\Users\\\\Richard\\\\Documents\\\\SEG4300\\\\Project\\\\SEG4300-Project\\\\Code\\\\models\\\\multihead_pitch_prediction.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import data_preprocessing\n",
    "import data_set_and_loader\n",
    "import models.multihead_pitch_prediction\n",
    "import pandas as pd\n",
    "from layers.lstm import LSTM\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(data_set_and_loader)\n",
    "importlib.reload(models.multihead_pitch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from models.multihead_pitch_prediction import MultiHeadLSTM\n",
    "from data_preprocessing import load_data, sort_n_group, build_seqs, encode_and_scale, compute_feature_medians, split_data\n",
    "from data_set_and_loader import create_dataloaders, AtBatDataset\n",
    "from config import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sort_n_group(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values = compute_feature_medians(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'release_speed': np.float64(90.3), 'release_spin_rate': np.float64(2274.0), 'release_extension': np.float64(6.3), 'plate_x': np.float64(0.04), 'plate_z': np.float64(2.28)}\n"
     ]
    }
   ],
   "source": [
    "print(median_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished sequence building. Dropped 52454 at-bats with NaN values.\n",
      "Dropped 145305 at-bats due to outliers.\n",
      "Dropped 8496 at-bats due to invalid results: {'strikeout', 'strikeout_double_play', 'walk', 'truncated_pa'}\n"
     ]
    }
   ],
   "source": [
    "X_sequences , Y_sequences = build_seqs(df, median_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_indices = [8, 9, 16, 17, 18, 19, 20]\n",
    "cat_indices = [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 21, 22]\n",
    "\n",
    "processed_X, processed_Y, label_encoders_X, y_type_encoder, y_desc_encoder, y_event_encoder = encode_and_scale(X_sequences, Y_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Target Label Encoders Mappings (Limited View) ===\n",
      "\n",
      "Pitch Type (y_type_encoder) - Total Unique Classes: 15\n",
      "   AB -> 0\n",
      "   CH -> 1\n",
      "   CS -> 2\n",
      "   CU -> 3\n",
      "   EP -> 4\n",
      "   FA -> 5\n",
      "   FC -> 6\n",
      "   FF -> 7\n",
      "   FS -> 8\n",
      "   KC -> 9\n",
      "   KN -> 10\n",
      "   SI -> 11\n",
      "   SL -> 12\n",
      "   ST -> 13\n",
      "   SV -> 14\n",
      "\n",
      "Result Description (y_desc_encoder) - Total Unique Classes: 13\n",
      "   ball -> 0\n",
      "   blocked_ball -> 1\n",
      "   bunt_foul_tip -> 2\n",
      "   called_strike -> 3\n",
      "   foul -> 4\n",
      "   foul_bunt -> 5\n",
      "   foul_tip -> 6\n",
      "   hit_by_pitch -> 7\n",
      "   hit_into_play -> 8\n",
      "   intent_ball -> 9\n",
      "   missed_bunt -> 10\n",
      "   swinging_strike -> 11\n",
      "   swinging_strike_blocked -> 12\n",
      "\n",
      "Result Event (y_event_encoder) - Total Unique Classes: 23\n",
      "   IN_PROGRESS -> 0\n",
      "   catcher_interf -> 1\n",
      "   double -> 2\n",
      "   double_play -> 3\n",
      "   field_error -> 4\n",
      "   field_out -> 5\n",
      "   fielders_choice -> 6\n",
      "   fielders_choice_out -> 7\n",
      "   force_out -> 8\n",
      "   grounded_into_double_play -> 9\n",
      "   hit_by_pitch -> 10\n",
      "   home_run -> 11\n",
      "   sac_bunt -> 12\n",
      "   sac_bunt_double_play -> 13\n",
      "   sac_fly -> 14\n",
      "   sac_fly_double_play -> 15\n",
      "   single -> 16\n",
      "   strikeout -> 17\n",
      "   strikeout_double_play -> 18\n",
      "   triple -> 19\n",
      "   triple_play -> 20\n",
      "   truncated_pa -> 21\n",
      "   walk -> 22\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Encoders for Y targets: pitch type, result description, and result event\n",
    "y_encoders = {\n",
    "    \"Pitch Type (y_type_encoder)\": y_type_encoder,\n",
    "    \"Result Description (y_desc_encoder)\": y_desc_encoder,\n",
    "    \"Result Event (y_event_encoder)\": y_event_encoder,\n",
    "}\n",
    "\n",
    "max_display = 23  # How many mappings to show per encoder\n",
    "\n",
    "print(\"\\n=== Target Label Encoders Mappings (Limited View) ===\")\n",
    "for name, encoder in y_encoders.items():\n",
    "    mapping_dict = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    \n",
    "    print(f\"\\n{name} - Total Unique Classes: {len(mapping_dict)}\")\n",
    "    \n",
    "    for i, (original, encoded) in enumerate(mapping_dict.items()):\n",
    "        if i >= max_display:\n",
    "            print(\"   ... (truncated)\")\n",
    "            break\n",
    "        print(f\"   {original} -> {encoded}\")\n",
    "    \n",
    "    # Optional: Store for debugging or reverse lookup\n",
    "    full_mapping = mapping_dict\n",
    "\n",
    "print(\"\\n===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = split_data(processed_X, processed_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_dataloaders(X_train, Y_train, X_test, Y_test, 128) #remember to adjust batch-size here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_indices = [8, 9, 16, 17, 18, 19, 20]\n",
    "x_low_card_cats_indices = [0, 1, 2, 3, 4, 5, 7, 10, 11, 14]\n",
    "x_inning_index = 6\n",
    "x_pitcher_index = 12\n",
    "x_batter_index = 13\n",
    "x_prev_pitch_index = 15\n",
    "x_prev_desc_index = 22\n",
    "x_prev_event_index = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/30] - Loss: 5.0409 - Pitch Type Acc: 44.39% - Desc Acc: 37.17% - Event Acc: 50.11% - Time: 397.62s\n",
      "Validation Loss: 4.7744 - Pitch Type Acc: 47.37% - Desc Acc: 37.74% - Event Acc: 51.12%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [2/30] - Loss: 4.7651 - Pitch Type Acc: 47.35% - Desc Acc: 37.84% - Event Acc: 51.23% - Time: 395.46s\n",
      "Validation Loss: 4.7251 - Pitch Type Acc: 47.97% - Desc Acc: 38.01% - Event Acc: 51.40%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [3/30] - Loss: 4.7385 - Pitch Type Acc: 47.74% - Desc Acc: 37.97% - Event Acc: 51.39% - Time: 396.36s\n",
      "Validation Loss: 4.7130 - Pitch Type Acc: 48.14% - Desc Acc: 38.06% - Event Acc: 51.46%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [4/30] - Loss: 4.6983 - Pitch Type Acc: 48.40% - Desc Acc: 38.19% - Event Acc: 51.59% - Time: 401.18s\n",
      "Validation Loss: 4.6780 - Pitch Type Acc: 48.79% - Desc Acc: 38.19% - Event Acc: 51.59%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [5/30] - Loss: 4.6921 - Pitch Type Acc: 48.51% - Desc Acc: 38.24% - Event Acc: 51.62% - Time: 394.57s\n",
      "Validation Loss: 4.6738 - Pitch Type Acc: 48.92% - Desc Acc: 38.24% - Event Acc: 51.62%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [6/30] - Loss: 4.6888 - Pitch Type Acc: 48.58% - Desc Acc: 38.26% - Event Acc: 51.66% - Time: 392.66s\n",
      "Validation Loss: 4.6705 - Pitch Type Acc: 48.97% - Desc Acc: 38.27% - Event Acc: 51.61%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [7/30] - Loss: 4.6658 - Pitch Type Acc: 48.96% - Desc Acc: 38.41% - Event Acc: 51.78% - Time: 393.65s\n",
      "Validation Loss: 4.6518 - Pitch Type Acc: 49.29% - Desc Acc: 38.38% - Event Acc: 51.66%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [8/30] - Loss: 4.6623 - Pitch Type Acc: 49.01% - Desc Acc: 38.43% - Event Acc: 51.82% - Time: 393.08s\n",
      "Validation Loss: 4.6505 - Pitch Type Acc: 49.33% - Desc Acc: 38.38% - Event Acc: 51.65%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [9/30] - Loss: 4.6602 - Pitch Type Acc: 49.07% - Desc Acc: 38.43% - Event Acc: 51.83% - Time: 392.97s\n",
      "Validation Loss: 4.6489 - Pitch Type Acc: 49.32% - Desc Acc: 38.39% - Event Acc: 51.65%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [10/30] - Loss: 4.6458 - Pitch Type Acc: 49.29% - Desc Acc: 38.54% - Event Acc: 51.93% - Time: 393.42s\n",
      "Validation Loss: 4.6381 - Pitch Type Acc: 49.55% - Desc Acc: 38.45% - Event Acc: 51.74%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [11/30] - Loss: 4.6434 - Pitch Type Acc: 49.33% - Desc Acc: 38.55% - Event Acc: 51.93% - Time: 394.08s\n",
      "Validation Loss: 4.6372 - Pitch Type Acc: 49.57% - Desc Acc: 38.47% - Event Acc: 51.73%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [12/30] - Loss: 4.6418 - Pitch Type Acc: 49.35% - Desc Acc: 38.56% - Event Acc: 51.95% - Time: 392.26s\n",
      "Validation Loss: 4.6361 - Pitch Type Acc: 49.59% - Desc Acc: 38.48% - Event Acc: 51.72%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [13/30] - Loss: 4.6325 - Pitch Type Acc: 49.50% - Desc Acc: 38.64% - Event Acc: 52.01% - Time: 392.07s\n",
      "Validation Loss: 4.6297 - Pitch Type Acc: 49.70% - Desc Acc: 38.50% - Event Acc: 51.80%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [14/30] - Loss: 4.6310 - Pitch Type Acc: 49.54% - Desc Acc: 38.65% - Event Acc: 52.02% - Time: 392.76s\n",
      "Validation Loss: 4.6289 - Pitch Type Acc: 49.71% - Desc Acc: 38.51% - Event Acc: 51.82%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [15/30] - Loss: 4.6300 - Pitch Type Acc: 49.52% - Desc Acc: 38.64% - Event Acc: 52.03% - Time: 393.17s\n",
      "Validation Loss: 4.6286 - Pitch Type Acc: 49.71% - Desc Acc: 38.51% - Event Acc: 51.82%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [16/30] - Loss: 4.6243 - Pitch Type Acc: 49.63% - Desc Acc: 38.67% - Event Acc: 52.08% - Time: 393.30s\n",
      "Validation Loss: 4.6253 - Pitch Type Acc: 49.77% - Desc Acc: 38.54% - Event Acc: 51.86%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [17/30] - Loss: 4.6233 - Pitch Type Acc: 49.63% - Desc Acc: 38.69% - Event Acc: 52.07% - Time: 392.47s\n",
      "Validation Loss: 4.6247 - Pitch Type Acc: 49.78% - Desc Acc: 38.55% - Event Acc: 51.88%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [18/30] - Loss: 4.6227 - Pitch Type Acc: 49.65% - Desc Acc: 38.69% - Event Acc: 52.10% - Time: 393.48s\n",
      "Validation Loss: 4.6243 - Pitch Type Acc: 49.78% - Desc Acc: 38.55% - Event Acc: 51.88%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [19/30] - Loss: 4.6199 - Pitch Type Acc: 49.70% - Desc Acc: 38.72% - Event Acc: 52.11% - Time: 393.18s\n",
      "Validation Loss: 4.6231 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.89%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [20/30] - Loss: 4.6192 - Pitch Type Acc: 49.72% - Desc Acc: 38.72% - Event Acc: 52.11% - Time: 393.34s\n",
      "Validation Loss: 4.6228 - Pitch Type Acc: 49.81% - Desc Acc: 38.57% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [21/30] - Loss: 4.6184 - Pitch Type Acc: 49.71% - Desc Acc: 38.72% - Event Acc: 52.13% - Time: 394.05s\n",
      "Validation Loss: 4.6227 - Pitch Type Acc: 49.81% - Desc Acc: 38.58% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [22/30] - Loss: 4.6169 - Pitch Type Acc: 49.74% - Desc Acc: 38.72% - Event Acc: 52.16% - Time: 393.64s\n",
      "Validation Loss: 4.6222 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [23/30] - Loss: 4.6169 - Pitch Type Acc: 49.75% - Desc Acc: 38.74% - Event Acc: 52.16% - Time: 393.02s\n",
      "Validation Loss: 4.6221 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.91%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [24/30] - Loss: 4.6165 - Pitch Type Acc: 49.74% - Desc Acc: 38.74% - Event Acc: 52.18% - Time: 394.35s\n",
      "Validation Loss: 4.6220 - Pitch Type Acc: 49.81% - Desc Acc: 38.57% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [25/30] - Loss: 4.6154 - Pitch Type Acc: 49.76% - Desc Acc: 38.74% - Event Acc: 52.16% - Time: 394.43s\n",
      "Validation Loss: 4.6218 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.91%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [26/30] - Loss: 4.6152 - Pitch Type Acc: 49.76% - Desc Acc: 38.75% - Event Acc: 52.16% - Time: 393.71s\n",
      "Validation Loss: 4.6217 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.91%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [27/30] - Loss: 4.6152 - Pitch Type Acc: 49.75% - Desc Acc: 38.73% - Event Acc: 52.17% - Time: 393.89s\n",
      "Validation Loss: 4.6216 - Pitch Type Acc: 49.82% - Desc Acc: 38.57% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [28/30] - Loss: 4.6144 - Pitch Type Acc: 49.77% - Desc Acc: 38.75% - Event Acc: 52.20% - Time: 394.63s\n",
      "Validation Loss: 4.6215 - Pitch Type Acc: 49.83% - Desc Acc: 38.58% - Event Acc: 51.90%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [29/30] - Loss: 4.6147 - Pitch Type Acc: 49.77% - Desc Acc: 38.76% - Event Acc: 52.19% - Time: 395.15s\n",
      "Validation Loss: 4.6215 - Pitch Type Acc: 49.83% - Desc Acc: 38.58% - Event Acc: 51.89%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Epoch [30/30] - Loss: 4.6146 - Pitch Type Acc: 49.76% - Desc Acc: 38.74% - Event Acc: 52.18% - Time: 392.50s\n",
      "Validation Loss: 4.6215 - Pitch Type Acc: 49.83% - Desc Acc: 38.57% - Event Acc: 51.89%\n",
      "🔥 New best model found! Saving...\n",
      "Model config saved to C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\model_config.txt\n",
      "Training complete! Best model saved as C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\best_model.pth\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.002\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\best_model.pth\"\n",
    "history = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model (update parameters based on your dataset)\n",
    "model = MultiHeadLSTM(\n",
    "    #INCREASE DIMS??\n",
    "    num_numeric_features=7,\n",
    "    num_pitchers=879, num_batters=3096,\n",
    "    num_prev_descriptions=12, num_prev_events=3, num_prev_pitch_types=16,\n",
    "    num_low_card_cats=10, \n",
    "    num_innings=20, inning_emb_dim=4,\n",
    "    pitcher_emb_dim=30, batter_emb_dim=50,\n",
    "    prev_description_emb_dim=4, prev_event_emb_dim=2, prev_pitch_emb_dim=4,\n",
    "    hidden_dim=128,\n",
    "    num_pitch_type_classes=16, num_description_classes=14, num_event_classes=24,\n",
    "    cont_dim=5, lstm_layers=2, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss functions\n",
    "criterion_classification = nn.CrossEntropyLoss(ignore_index=0)\n",
    "criterion_continuous = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler: Reduce LR by 50% every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Track best validation loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "### 🚀 Training Function\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss, total_pitch_type_loss, total_desc_loss, total_event_loss, total_cont_loss = 0, 0, 0, 0, 0\n",
    "    correct_pitch_types, correct_desc, correct_event = 0, 0, 0\n",
    "    total_pitch_types, total_desc, total_event = 0, 0, 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        (\n",
    "            padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths\n",
    "        ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "        # Extract features based on new indices\n",
    "        x_num = padded_X[:, :, x_num_indices]  # Numeric features\n",
    "        x_low_card_cats = padded_X[:, :, x_low_card_cats_indices]  # Low-cardinality categorical\n",
    "        x_inning = padded_X[:, :, x_inning_index].long()  # Inning (integer categories)\n",
    "        x_pitcher = padded_X[:, :, x_pitcher_index].long()\n",
    "        x_batter = padded_X[:, :, x_batter_index].long()\n",
    "        x_prev_pitch = padded_X[:, :, x_prev_pitch_index].long()\n",
    "        x_prev_desc = padded_X[:, :, x_prev_desc_index].long()\n",
    "        x_prev_event = padded_X[:, :, x_prev_event_index].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pitch_type_logits, pitch_cont_values, pitch_result_desc, pitch_result_event = model(\n",
    "            x_num, x_low_card_cats, x_inning, x_pitcher, x_batter, x_prev_desc, x_prev_event, x_prev_pitch, lengths\n",
    "        )\n",
    "\n",
    "        # Compute losses\n",
    "        loss_pitch_type = criterion_classification(\n",
    "            pitch_type_logits.view(-1, pitch_type_logits.size(-1)), padded_Y_type.view(-1)\n",
    "        )\n",
    "        loss_desc = criterion_classification(\n",
    "            pitch_result_desc.view(-1, pitch_result_desc.size(-1)), padded_Y_desc.view(-1)\n",
    "        )\n",
    "        loss_event = criterion_classification(\n",
    "            pitch_result_event.view(-1, pitch_result_event.size(-1)), padded_Y_event.view(-1)\n",
    "        )\n",
    "        #Handle padded numerical values (-999) in MSE loss\n",
    "        mask = (padded_Y_cont != -999).float()  # Create a mask (1 for real values, 0 for padded)\n",
    "        loss_cont = (mask * (pitch_cont_values - padded_Y_cont) ** 2).sum() / mask.sum()\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_pitch_type + loss_desc + loss_event + loss_cont\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        total_pitch_type_loss += loss_pitch_type.item()\n",
    "        total_desc_loss += loss_desc.item()\n",
    "        total_event_loss += loss_event.item()\n",
    "        total_cont_loss += loss_cont.item()\n",
    "\n",
    "        # Compute accuracy for all categorical outputs (ignore padding)\n",
    "        mask_type = (padded_Y_type != 0)  \n",
    "        mask_desc = (padded_Y_desc != 0)\n",
    "        mask_event = (padded_Y_event != 0)\n",
    "\n",
    "        _, predicted_pitch_type = torch.max(pitch_type_logits, dim=-1)\n",
    "        _, predicted_desc = torch.max(pitch_result_desc, dim=-1)\n",
    "        _, predicted_event = torch.max(pitch_result_event, dim=-1)\n",
    "\n",
    "        correct_pitch_types += (predicted_pitch_type[mask_type] == padded_Y_type[mask_type]).sum().item()\n",
    "        correct_desc += (predicted_desc[mask_desc] == padded_Y_desc[mask_desc]).sum().item()\n",
    "        correct_event += (predicted_event[mask_event] == padded_Y_event[mask_event]).sum().item()\n",
    "\n",
    "        total_pitch_types += mask_type.sum().item()\n",
    "        total_desc += mask_desc.sum().item()\n",
    "        total_event += mask_event.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    pitch_type_acc = 100 * correct_pitch_types / total_pitch_types\n",
    "    desc_acc = 100 * correct_desc / total_desc\n",
    "    event_acc = 100 * correct_event / total_event\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f} - Pitch Type Acc: {pitch_type_acc:.2f}% - Desc Acc: {desc_acc:.2f}% - Event Acc: {event_acc:.2f}% - Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    history.append({\n",
    "        \"train_pitch_type_acc\": pitch_type_acc,\n",
    "        \"train_desc_acc\": desc_acc,\n",
    "        \"train_event_acc\": event_acc\n",
    "    })\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "### 🚀 Validation Function\n",
    "def validate():\n",
    "    model.eval()\n",
    "    val_loss, val_pitch_type_loss, val_desc_loss, val_event_loss, val_cont_loss = 0, 0, 0, 0, 0\n",
    "    correct_pitch_types, correct_desc, correct_event = 0, 0, 0\n",
    "    total_pitch_types, total_desc, total_event = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            (\n",
    "                padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths\n",
    "            ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            # Extract features based on new indices\n",
    "            x_num = padded_X[:, :, x_num_indices]  # Numeric features\n",
    "            x_low_card_cats = padded_X[:, :, x_low_card_cats_indices]  # Low-cardinality categorical\n",
    "            x_inning = padded_X[:, :, x_inning_index].long()  # Inning (integer categories)\n",
    "            x_pitcher = padded_X[:, :, x_pitcher_index].long()\n",
    "            x_batter = padded_X[:, :, x_batter_index].long()\n",
    "            x_prev_pitch = padded_X[:, :, x_prev_pitch_index].long()\n",
    "            x_prev_desc = padded_X[:, :, x_prev_desc_index].long()\n",
    "            x_prev_event = padded_X[:, :, x_prev_event_index].long()\n",
    "\n",
    "            pitch_type_logits, pitch_cont_values, pitch_result_desc, pitch_result_event = model(\n",
    "                x_num, x_low_card_cats, x_inning, x_pitcher, x_batter, x_prev_desc, x_prev_event, x_prev_pitch, lengths\n",
    "            )\n",
    "\n",
    "            # Compute validation losses\n",
    "            loss_pitch_type = criterion_classification(\n",
    "                pitch_type_logits.view(-1, pitch_type_logits.size(-1)), padded_Y_type.view(-1)\n",
    "            )\n",
    "            loss_desc = criterion_classification(\n",
    "                pitch_result_desc.view(-1, pitch_result_desc.size(-1)), padded_Y_desc.view(-1)\n",
    "            )\n",
    "            loss_event = criterion_classification(\n",
    "                pitch_result_event.view(-1, pitch_result_event.size(-1)), padded_Y_event.view(-1)\n",
    "            )\n",
    "            mask = (padded_Y_cont != -999).float()\n",
    "            loss_cont = (mask * (pitch_cont_values - padded_Y_cont) ** 2).sum() / mask.sum()\n",
    "\n",
    "            loss = loss_pitch_type + loss_desc + loss_event + loss_cont\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy for all categorical outputs (ignore padding)\n",
    "            mask_type = (padded_Y_type != 0)  \n",
    "            mask_desc = (padded_Y_desc != 0)\n",
    "            mask_event = (padded_Y_event != 0)\n",
    "\n",
    "            _, predicted_pitch_type = torch.max(pitch_type_logits, dim=-1)\n",
    "            _, predicted_desc = torch.max(pitch_result_desc, dim=-1)\n",
    "            _, predicted_event = torch.max(pitch_result_event, dim=-1)\n",
    "\n",
    "            correct_pitch_types += (predicted_pitch_type[mask_type] == padded_Y_type[mask_type]).sum().item()\n",
    "            correct_desc += (predicted_desc[mask_desc] == padded_Y_desc[mask_desc]).sum().item()\n",
    "            correct_event += (predicted_event[mask_event] == padded_Y_event[mask_event]).sum().item()\n",
    "\n",
    "            total_pitch_types += mask_type.sum().item()\n",
    "            total_desc += mask_desc.sum().item()\n",
    "            total_event += mask_event.sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    pitch_type_acc = 100 * correct_pitch_types / total_pitch_types\n",
    "    desc_acc = 100 * correct_desc / total_desc\n",
    "    event_acc = 100 * correct_event / total_event\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} - Pitch Type Acc: {pitch_type_acc:.2f}% - Desc Acc: {desc_acc:.2f}% - Event Acc: {event_acc:.2f}%\")\n",
    "\n",
    "    history.append({\n",
    "        \"val_pitch_type_acc\": pitch_type_acc,\n",
    "        \"val_desc_acc\": desc_acc,\n",
    "        \"val_event_acc\": event_acc\n",
    "    })\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "early_stopping_patience = 5\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "### 🚀 Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(epoch)\n",
    "    val_loss = validate()\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"🔥 New best model found! Saving...\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        model.save_model_config()\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "df_history = pd.DataFrame(history)\n",
    "df_history.to_csv(r\"C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\training_metrics.csv\", index=False)\n",
    "print(\"Training complete! Best model saved as\", MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention(model, sample_batch, lengths):\n",
    "    \"\"\"\n",
    "    Visualizes the LSTM's attention over past pitches using hidden states.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths\n",
    "        ) = [tensor.to(device) for tensor in sample_batch]\n",
    "\n",
    "        # Extract features based on new indices\n",
    "        x_num = padded_X[:, :, x_num_indices]  \n",
    "        x_low_card_cats = padded_X[:, :, x_low_card_cats_indices]\n",
    "        x_inning = padded_X[:, :, x_inning_index].long()  \n",
    "        x_pitcher = padded_X[:, :, x_pitcher_index].long()\n",
    "        x_batter = padded_X[:, :, x_batter_index].long()\n",
    "        x_prev_pitch = padded_X[:, :, x_prev_pitch_index].long()\n",
    "        x_prev_desc = padded_X[:, :, x_prev_desc_index].long()\n",
    "        x_prev_event = padded_X[:, :, x_prev_event_index].long()\n",
    "\n",
    "        # Forward pass to extract LSTM hidden states\n",
    "        _, (h_n, c_n) = model.lstm(\n",
    "            torch.cat([x_num, x_low_card_cats], dim=2)\n",
    "        )\n",
    "\n",
    "        attention_scores = torch.mean(h_n, dim=0).cpu().numpy()\n",
    "\n",
    "    # Plot attention over sequence length\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(attention_scores.T, cmap=\"viridis\", annot=False)\n",
    "    plt.xlabel(\"LSTM Layer\")\n",
    "    plt.ylabel(\"Sequence Length\")\n",
    "    plt.title(\"LSTM Hidden State Attention Heatmap\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_embeddings(embedding_matrix, labels, title):\n",
    "    \"\"\"\n",
    "    Visualizes high-dimensional learned embeddings using t-SNE.\n",
    "    \"\"\"\n",
    "    num_samples = embedding_matrix.shape[0]  # Number of embeddings\n",
    "    perplexity = min(5, num_samples - 1)  # Ensure perplexity < num_samples\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embedded_2d = tsne.fit_transform(embedding_matrix.cpu().detach().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(embedded_2d[:, 0], embedded_2d[:, 1], alpha=0.7, cmap=\"viridis\")\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        plt.text(embedded_2d[i, 0], embedded_2d[i, 1], str(label), fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Extract learned embeddings\n",
    "def inspect_learned_embeddings():\n",
    "    \"\"\"\n",
    "    Extracts and visualizes learned embeddings for pitchers, batters, and innings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pitcher_embeddings = model.pitcher_emb.weight\n",
    "    batter_embeddings = model.batter_emb.weight\n",
    "    inning_embeddings = model.inning_emb.weight\n",
    "    prev_desc_embs = model.prev_description_emb.weight\n",
    "    prev_event_embs = model.prev_event_emb.weight\n",
    "    prev_pitch_embs = model.prev_pitch_emb.weight\n",
    "\n",
    "    plot_embeddings(pitcher_embeddings, range(len(pitcher_embeddings)), \"Pitcher Embeddings\")\n",
    "    plot_embeddings(batter_embeddings, range(len(batter_embeddings)), \"Batter Embeddings\")\n",
    "    plot_embeddings(inning_embeddings, range(len(inning_embeddings)), \"Inning Embeddings\")\n",
    "    plot_embeddings(prev_desc_embs, range(len(prev_desc_embs)), \"Prev Desc Embeddings\")\n",
    "    plot_embeddings(prev_event_embs, range(len(prev_event_embs)), \"Prev Event Embeddings\")\n",
    "    plot_embeddings(prev_pitch_embs, range(len(prev_pitch_embs)), \"Prev Pitch Embeddings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM attention for a random batch\n",
    "#sample_batch = next(iter(test_loader))\n",
    "#visualize_attention(model, sample_batch, lengths)\n",
    "\n",
    "# Inspect learned pitcher/batter/inning embeddings\n",
    "inspect_learned_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def compute_embedding_similarity(embedding_matrix, labels, title=\"Embedding Similarity\"):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between embeddings and visualize it.\n",
    "    \n",
    "    embedding_matrix: torch.Tensor (num_classes, embedding_dim)\n",
    "    labels: list of class labels\n",
    "    title: Title for the heatmap\n",
    "    \"\"\"\n",
    "    # Convert embeddings to numpy\n",
    "    embeddings = embedding_matrix.cpu().detach().numpy()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    similarity_matrix /= (norms @ norms.T)  # Normalize\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=False, cmap=\"coolwarm\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Extract learned embeddings from the trained model\n",
    "event_embeddings = model.prev_event_emb.weight  # Shape: (num_events, emb_dim)\n",
    "pitcher_embeddings = model.pitcher_emb.weight\n",
    "batter_embeddings = model.batter_emb.weight\n",
    "inning_embeddings = model.inning_emb.weight\n",
    "prev_desc_embs = model.prev_description_emb.weight\n",
    "prev_pitch_embs = model.prev_pitch_emb.weight\n",
    "\n",
    "# Define labels (Replace with actual event names if available)\n",
    "event_labels = [f\"Event {i}\" for i in range(event_embeddings.shape[0])]\n",
    "pitcher_labels = [f\"Pitcher {i}\" for i in range(pitcher_embeddings.shape[0])]\n",
    "batter_labels = [f\"Batter {i}\" for i in range(batter_embeddings.shape[0])]\n",
    "inning_labels = [f\"Inning {i}\" for i in range(inning_embeddings.shape[0])]\n",
    "prev_desc_labels = [f\"Event {i}\" for i in range(prev_desc_embs.shape[0])]\n",
    "prev_pitch_labels = [f\"Event {i}\" for i in range(prev_pitch_embs.shape[0])]\n",
    "\n",
    "# Compute and visualize event embedding similarity\n",
    "compute_embedding_similarity(event_embeddings, event_labels, title=\"Event Embedding Similarity\")\n",
    "compute_embedding_similarity(pitcher_embeddings, pitcher_labels, title=\"Pitcher Embedding Similarity\")\n",
    "compute_embedding_similarity(batter_embeddings, batter_labels, title=\"Batter Embedding Similarity\")\n",
    "compute_embedding_similarity(inning_embeddings, inning_labels, title=\"Inning Embedding Similarity\")\n",
    "compute_embedding_similarity(prev_desc_embs, prev_desc_labels, title=\"Prev Desc Embedding Similarity\")\n",
    "compute_embedding_similarity(prev_pitch_embs, prev_pitch_labels, title=\"Prev Pitch Embedding Similarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/25] - Loss: 5.3140 - Pitch Type Acc: 41.85% - Desc Acc: 37.11% - Event Acc: 49.61% - Time: 458.69s\n",
      "Validation Loss: 4.9667 - Pitch Type Acc: 45.89% - Desc Acc: 37.48% - Event Acc: 50.55%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [2/25] - Loss: 4.8973 - Pitch Type Acc: 46.38% - Desc Acc: 37.58% - Event Acc: 50.74% - Time: 450.39s\n",
      "Validation Loss: 4.8072 - Pitch Type Acc: 47.45% - Desc Acc: 37.70% - Event Acc: 51.04%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [3/25] - Loss: 4.7997 - Pitch Type Acc: 47.29% - Desc Acc: 37.80% - Event Acc: 51.16% - Time: 444.68s\n",
      "Validation Loss: 4.7519 - Pitch Type Acc: 47.99% - Desc Acc: 37.76% - Event Acc: 51.35%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [4/25] - Loss: 4.7571 - Pitch Type Acc: 47.81% - Desc Acc: 37.96% - Event Acc: 51.43% - Time: 444.27s\n",
      "Validation Loss: 4.7243 - Pitch Type Acc: 48.34% - Desc Acc: 37.91% - Event Acc: 51.42%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [5/25] - Loss: 4.7312 - Pitch Type Acc: 48.09% - Desc Acc: 38.09% - Event Acc: 51.60% - Time: 442.77s\n",
      "Validation Loss: 4.7071 - Pitch Type Acc: 48.57% - Desc Acc: 38.00% - Event Acc: 51.48%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [6/25] - Loss: 4.7128 - Pitch Type Acc: 48.39% - Desc Acc: 38.17% - Event Acc: 51.74% - Time: 444.10s\n",
      "Validation Loss: 4.6943 - Pitch Type Acc: 48.74% - Desc Acc: 38.07% - Event Acc: 51.55%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [7/25] - Loss: 4.6990 - Pitch Type Acc: 48.55% - Desc Acc: 38.24% - Event Acc: 51.82% - Time: 444.56s\n",
      "Validation Loss: 4.6853 - Pitch Type Acc: 48.89% - Desc Acc: 38.11% - Event Acc: 51.57%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [8/25] - Loss: 4.6877 - Pitch Type Acc: 48.73% - Desc Acc: 38.29% - Event Acc: 51.89% - Time: 444.86s\n",
      "Validation Loss: 4.6782 - Pitch Type Acc: 49.04% - Desc Acc: 38.17% - Event Acc: 51.62%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [9/25] - Loss: 4.6788 - Pitch Type Acc: 48.88% - Desc Acc: 38.36% - Event Acc: 51.98% - Time: 444.03s\n",
      "Validation Loss: 4.6728 - Pitch Type Acc: 49.15% - Desc Acc: 38.19% - Event Acc: 51.62%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [10/25] - Loss: 4.6707 - Pitch Type Acc: 48.99% - Desc Acc: 38.41% - Event Acc: 52.04% - Time: 445.96s\n",
      "Validation Loss: 4.6689 - Pitch Type Acc: 49.18% - Desc Acc: 38.22% - Event Acc: 51.66%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [11/25] - Loss: 4.6640 - Pitch Type Acc: 49.08% - Desc Acc: 38.43% - Event Acc: 52.10% - Time: 444.45s\n",
      "Validation Loss: 4.6655 - Pitch Type Acc: 49.24% - Desc Acc: 38.24% - Event Acc: 51.68%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [12/25] - Loss: 4.6585 - Pitch Type Acc: 49.17% - Desc Acc: 38.49% - Event Acc: 52.16% - Time: 445.16s\n",
      "Validation Loss: 4.6632 - Pitch Type Acc: 49.28% - Desc Acc: 38.26% - Event Acc: 51.65%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [13/25] - Loss: 4.6533 - Pitch Type Acc: 49.27% - Desc Acc: 38.52% - Event Acc: 52.22% - Time: 446.29s\n",
      "Validation Loss: 4.6623 - Pitch Type Acc: 49.32% - Desc Acc: 38.22% - Event Acc: 51.60%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [14/25] - Loss: 4.6488 - Pitch Type Acc: 49.30% - Desc Acc: 38.57% - Event Acc: 52.31% - Time: 445.40s\n",
      "Validation Loss: 4.6603 - Pitch Type Acc: 49.38% - Desc Acc: 38.26% - Event Acc: 51.63%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [15/25] - Loss: 4.6450 - Pitch Type Acc: 49.35% - Desc Acc: 38.60% - Event Acc: 52.37% - Time: 446.74s\n",
      "Validation Loss: 4.6589 - Pitch Type Acc: 49.41% - Desc Acc: 38.24% - Event Acc: 51.61%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [16/25] - Loss: 4.6408 - Pitch Type Acc: 49.41% - Desc Acc: 38.64% - Event Acc: 52.42% - Time: 446.46s\n",
      "Validation Loss: 4.6591 - Pitch Type Acc: 49.38% - Desc Acc: 38.24% - Event Acc: 51.53%\n",
      "Epoch [17/25] - Loss: 4.6372 - Pitch Type Acc: 49.44% - Desc Acc: 38.66% - Event Acc: 52.48% - Time: 446.72s\n",
      "Validation Loss: 4.6583 - Pitch Type Acc: 49.41% - Desc Acc: 38.23% - Event Acc: 51.53%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [18/25] - Loss: 4.6343 - Pitch Type Acc: 49.49% - Desc Acc: 38.70% - Event Acc: 52.55% - Time: 446.40s\n",
      "Validation Loss: 4.6583 - Pitch Type Acc: 49.37% - Desc Acc: 38.23% - Event Acc: 51.51%\n",
      "🔥 New best model found! Saving...\n",
      "Epoch [19/25] - Loss: 4.6310 - Pitch Type Acc: 49.50% - Desc Acc: 38.72% - Event Acc: 52.61% - Time: 443.70s\n",
      "Validation Loss: 4.6601 - Pitch Type Acc: 49.40% - Desc Acc: 38.23% - Event Acc: 51.52%\n",
      "Epoch [20/25] - Loss: 4.6282 - Pitch Type Acc: 49.54% - Desc Acc: 38.76% - Event Acc: 52.71% - Time: 445.17s\n",
      "Validation Loss: 4.6617 - Pitch Type Acc: 49.41% - Desc Acc: 38.21% - Event Acc: 51.45%\n",
      "Epoch [21/25] - Loss: 4.6257 - Pitch Type Acc: 49.55% - Desc Acc: 38.79% - Event Acc: 52.78% - Time: 445.52s\n",
      "Validation Loss: 4.6612 - Pitch Type Acc: 49.42% - Desc Acc: 38.20% - Event Acc: 51.39%\n",
      "Epoch [22/25] - Loss: 4.6232 - Pitch Type Acc: 49.59% - Desc Acc: 38.84% - Event Acc: 52.85% - Time: 447.50s\n",
      "Validation Loss: 4.6625 - Pitch Type Acc: 49.41% - Desc Acc: 38.22% - Event Acc: 51.33%\n",
      "Epoch [23/25] - Loss: 4.6207 - Pitch Type Acc: 49.61% - Desc Acc: 38.85% - Event Acc: 52.92% - Time: 446.00s\n",
      "Validation Loss: 4.6631 - Pitch Type Acc: 49.46% - Desc Acc: 38.20% - Event Acc: 51.30%\n",
      "Early stopping triggered\n",
      "Training complete! Best model saved as C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\best_model.pth\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\Richard\\Documents\\SEG4300\\Project\\SEG4300-Project\\Code\\saved_models\\best_model.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model (update parameters based on your dataset)\n",
    "model = MultiHeadLSTM(\n",
    "    num_numeric_features=7,\n",
    "    num_pitchers=879, num_batters=3096,\n",
    "    num_prev_descriptions=12, num_prev_events=3, num_prev_pitch_types=16,\n",
    "    num_low_card_cats=10, \n",
    "    num_innings=20, inning_emb_dim=3,\n",
    "    pitcher_emb_dim=9, batter_emb_dim=12,\n",
    "    prev_description_emb_dim=3, prev_event_emb_dim=2, prev_pitch_emb_dim=3,\n",
    "    hidden_dim=128,\n",
    "    num_pitch_type_classes=16, num_description_classes=14, num_event_classes=24,\n",
    "    cont_dim=5, lstm_layers=2, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss functions\n",
    "criterion_classification = nn.CrossEntropyLoss(ignore_index=0)\n",
    "criterion_continuous = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Track best validation loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "### 🚀 Training Function\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss, total_pitch_type_loss, total_desc_loss, total_event_loss, total_cont_loss = 0, 0, 0, 0, 0\n",
    "    correct_pitch_types, correct_desc, correct_event = 0, 0, 0\n",
    "    total_pitch_types, total_desc, total_event = 0, 0, 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        (\n",
    "            padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths\n",
    "        ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "        # Extract features based on new indices\n",
    "        x_num = padded_X[:, :, x_num_indices]  # Numeric features\n",
    "        x_low_card_cats = padded_X[:, :, x_low_card_cats_indices]  # Low-cardinality categorical\n",
    "        x_inning = padded_X[:, :, x_inning_index].long()  # Inning (integer categories)\n",
    "        x_pitcher = padded_X[:, :, x_pitcher_index].long()\n",
    "        x_batter = padded_X[:, :, x_batter_index].long()\n",
    "        x_prev_pitch = padded_X[:, :, x_prev_pitch_index].long()\n",
    "        x_prev_desc = padded_X[:, :, x_prev_desc_index].long()\n",
    "        x_prev_event = padded_X[:, :, x_prev_event_index].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pitch_type_logits, pitch_cont_values, pitch_result_desc, pitch_result_event = model(\n",
    "            x_num, x_low_card_cats, x_inning, x_pitcher, x_batter, x_prev_desc, x_prev_event, x_prev_pitch, lengths\n",
    "        )\n",
    "\n",
    "        # Compute losses\n",
    "        loss_pitch_type = criterion_classification(\n",
    "            pitch_type_logits.view(-1, pitch_type_logits.size(-1)), padded_Y_type.view(-1)\n",
    "        )\n",
    "        loss_desc = criterion_classification(\n",
    "            pitch_result_desc.view(-1, pitch_result_desc.size(-1)), padded_Y_desc.view(-1)\n",
    "        )\n",
    "        loss_event = criterion_classification(\n",
    "            pitch_result_event.view(-1, pitch_result_event.size(-1)), padded_Y_event.view(-1)\n",
    "        )\n",
    "        #Handle padded numerical values (-999) in MSE loss\n",
    "        mask = (padded_Y_cont != -999).float()  # Create a mask (1 for real values, 0 for padded)\n",
    "        loss_cont = (mask * (pitch_cont_values - padded_Y_cont) ** 2).sum() / mask.sum()\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_pitch_type + loss_desc + loss_event + loss_cont\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        total_pitch_type_loss += loss_pitch_type.item()\n",
    "        total_desc_loss += loss_desc.item()\n",
    "        total_event_loss += loss_event.item()\n",
    "        total_cont_loss += loss_cont.item()\n",
    "\n",
    "        # Compute accuracy for all categorical outputs (ignore padding)\n",
    "        mask_type = (padded_Y_type != 0)  \n",
    "        mask_desc = (padded_Y_desc != 0)\n",
    "        mask_event = (padded_Y_event != 0)\n",
    "\n",
    "        _, predicted_pitch_type = torch.max(pitch_type_logits, dim=-1)\n",
    "        _, predicted_desc = torch.max(pitch_result_desc, dim=-1)\n",
    "        _, predicted_event = torch.max(pitch_result_event, dim=-1)\n",
    "\n",
    "        correct_pitch_types += (predicted_pitch_type[mask_type] == padded_Y_type[mask_type]).sum().item()\n",
    "        correct_desc += (predicted_desc[mask_desc] == padded_Y_desc[mask_desc]).sum().item()\n",
    "        correct_event += (predicted_event[mask_event] == padded_Y_event[mask_event]).sum().item()\n",
    "\n",
    "        total_pitch_types += mask_type.sum().item()\n",
    "        total_desc += mask_desc.sum().item()\n",
    "        total_event += mask_event.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    pitch_type_acc = 100 * correct_pitch_types / total_pitch_types\n",
    "    desc_acc = 100 * correct_desc / total_desc\n",
    "    event_acc = 100 * correct_event / total_event\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f} - Pitch Type Acc: {pitch_type_acc:.2f}% - Desc Acc: {desc_acc:.2f}% - Event Acc: {event_acc:.2f}% - Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "### 🚀 Validation Function\n",
    "def validate():\n",
    "    model.eval()\n",
    "    val_loss, val_pitch_type_loss, val_desc_loss, val_event_loss, val_cont_loss = 0, 0, 0, 0, 0\n",
    "    correct_pitch_types, correct_desc, correct_event = 0, 0, 0\n",
    "    total_pitch_types, total_desc, total_event = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            (\n",
    "                padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths\n",
    "            ) = [tensor.to(device) for tensor in batch]\n",
    "\n",
    "            # Extract features based on new indices\n",
    "            x_num = padded_X[:, :, x_num_indices]  # Numeric features\n",
    "            x_low_card_cats = padded_X[:, :, x_low_card_cats_indices]  # Low-cardinality categorical\n",
    "            x_inning = padded_X[:, :, x_inning_index].long()  # Inning (integer categories)\n",
    "            x_pitcher = padded_X[:, :, x_pitcher_index].long()\n",
    "            x_batter = padded_X[:, :, x_batter_index].long()\n",
    "            x_prev_pitch = padded_X[:, :, x_prev_pitch_index].long()\n",
    "            x_prev_desc = padded_X[:, :, x_prev_desc_index].long()\n",
    "            x_prev_event = padded_X[:, :, x_prev_event_index].long()\n",
    "\n",
    "            pitch_type_logits, pitch_cont_values, pitch_result_desc, pitch_result_event = model(\n",
    "                x_num, x_low_card_cats, x_inning, x_pitcher, x_batter, x_prev_desc, x_prev_event, x_prev_pitch, lengths\n",
    "            )\n",
    "\n",
    "            # Compute validation losses\n",
    "            loss_pitch_type = criterion_classification(\n",
    "                pitch_type_logits.view(-1, pitch_type_logits.size(-1)), padded_Y_type.view(-1)\n",
    "            )\n",
    "            loss_desc = criterion_classification(\n",
    "                pitch_result_desc.view(-1, pitch_result_desc.size(-1)), padded_Y_desc.view(-1)\n",
    "            )\n",
    "            loss_event = criterion_classification(\n",
    "                pitch_result_event.view(-1, pitch_result_event.size(-1)), padded_Y_event.view(-1)\n",
    "            )\n",
    "            mask = (padded_Y_cont != -999).float()\n",
    "            loss_cont = (mask * (pitch_cont_values - padded_Y_cont) ** 2).sum() / mask.sum()\n",
    "\n",
    "            loss = loss_pitch_type + loss_desc + loss_event + loss_cont\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy for all categorical outputs (ignore padding)\n",
    "            mask_type = (padded_Y_type != 0)  \n",
    "            mask_desc = (padded_Y_desc != 0)\n",
    "            mask_event = (padded_Y_event != 0)\n",
    "\n",
    "            _, predicted_pitch_type = torch.max(pitch_type_logits, dim=-1)\n",
    "            _, predicted_desc = torch.max(pitch_result_desc, dim=-1)\n",
    "            _, predicted_event = torch.max(pitch_result_event, dim=-1)\n",
    "\n",
    "            correct_pitch_types += (predicted_pitch_type[mask_type] == padded_Y_type[mask_type]).sum().item()\n",
    "            correct_desc += (predicted_desc[mask_desc] == padded_Y_desc[mask_desc]).sum().item()\n",
    "            correct_event += (predicted_event[mask_event] == padded_Y_event[mask_event]).sum().item()\n",
    "\n",
    "            total_pitch_types += mask_type.sum().item()\n",
    "            total_desc += mask_desc.sum().item()\n",
    "            total_event += mask_event.sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    pitch_type_acc = 100 * correct_pitch_types / total_pitch_types\n",
    "    desc_acc = 100 * correct_desc / total_desc\n",
    "    event_acc = 100 * correct_event / total_event\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} - Pitch Type Acc: {pitch_type_acc:.2f}% - Desc Acc: {desc_acc:.2f}% - Event Acc: {event_acc:.2f}%\")\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "early_stopping_patience = 5\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "### 🚀 Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(epoch)\n",
    "    val_loss = validate()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"🔥 New best model found! Saving...\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete! Best model saved as\", MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataloader(dataloader, dataset_name=\"Train\"):\n",
    "    \"\"\"\n",
    "    Analyzes a given DataLoader by inspecting batch shapes, categorical distributions, and numerical statistics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Analyzing {dataset_name} DataLoader...\\n\")\n",
    "    \n",
    "    # Storage for analysis\n",
    "    sequence_lengths = []\n",
    "    type_counts = Counter()\n",
    "    desc_counts = Counter()\n",
    "    event_counts = Counter()\n",
    "    cont_values = []\n",
    "    \n",
    "    batch_count = 0\n",
    "\n",
    "    # Iterate over a few batches to gather statistics\n",
    "    for batch in dataloader:\n",
    "        padded_X, padded_Y_type, padded_Y_cont, padded_Y_desc, padded_Y_event, lengths = batch\n",
    "        batch_size, max_seq_len, input_dim = padded_X.shape\n",
    "        \n",
    "        print(f\"🟢 Batch {batch_count+1}:\")\n",
    "        print(f\"  - padded_X shape: {padded_X.shape} (Batch: {batch_size}, Max Seq Length: {max_seq_len}, Features: {input_dim})\")\n",
    "        print(f\"  - padded_Y_type shape: {padded_Y_type.shape}\")\n",
    "        print(f\"  - padded_Y_cont shape: {padded_Y_cont.shape}\")\n",
    "        print(f\"  - padded_Y_desc shape: {padded_Y_desc.shape}\")\n",
    "        print(f\"  - padded_Y_event shape: {padded_Y_event.shape}\")\n",
    "        print(f\"  - Lengths tensor shape: {lengths.shape}\\n\")\n",
    "        \n",
    "        # Collect sequence lengths\n",
    "        sequence_lengths.extend(lengths.tolist())\n",
    "\n",
    "        # Collect categorical distributions\n",
    "        type_counts.update(padded_Y_type[padded_Y_type != -100].tolist())  # Ignore padding\n",
    "        desc_counts.update(padded_Y_desc[padded_Y_desc != -100].tolist())  \n",
    "        event_counts.update(padded_Y_event[padded_Y_event != -100].tolist())  \n",
    "\n",
    "        # Collect numerical Y_cont values\n",
    "        valid_Y_cont = padded_Y_cont[padded_Y_cont != -999]  # Ignore padding\n",
    "        if valid_Y_cont.numel() > 0:\n",
    "            cont_values.extend(valid_Y_cont.tolist()) \n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count >= 5:  # Analyze first 5 batches only\n",
    "            break  \n",
    "\n",
    "    # ✅ 1️⃣ Sequence Length Distribution\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(sequence_lengths, bins=range(1, max(sequence_lengths) + 1), alpha=0.7, label=dataset_name)\n",
    "    plt.xlabel(\"Sequence Length (# pitches per at-bat)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"📊 {dataset_name} Sequence Length Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ✅ 2️⃣ Target Categorical Distribution (Pitch Type, Description, Event)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    \n",
    "    axes[0].bar(type_counts.keys(), type_counts.values(), alpha=0.7)\n",
    "    axes[0].set_title(\"Pitch Type Distribution\")\n",
    "    axes[0].set_xlabel(\"Encoded Pitch Type\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    \n",
    "    axes[1].bar(desc_counts.keys(), desc_counts.values(), alpha=0.7, color=\"orange\")\n",
    "    axes[1].set_title(\"Pitch Description Distribution\")\n",
    "    axes[1].set_xlabel(\"Encoded Description\")\n",
    "    \n",
    "    axes[2].bar(event_counts.keys(), event_counts.values(), alpha=0.7, color=\"green\")\n",
    "    axes[2].set_title(\"Pitch Event Distribution\")\n",
    "    axes[2].set_xlabel(\"Encoded Event\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # ✅ 3️⃣ Y_cont Statistics\n",
    "    if cont_values:\n",
    "        cont_values = torch.tensor(cont_values)\n",
    "        mean_values = torch.mean(cont_values, dim=0)\n",
    "        std_values = torch.std(cont_values, dim=0)\n",
    "        print(\"\\n=== Numerical Feature Scaling Check (Y_cont) ===\")\n",
    "        print(f\"Mean values (should be ~0 after scaling): {mean_values.tolist()}\")\n",
    "        print(f\"Standard deviations (should be ~1 after scaling): {std_values.tolist()}\")\n",
    "        print(\"✅ Scaling verified.\\n\")\n",
    "\n",
    "    print(\"✅ DataLoader Analysis Completed!\\n\")\n",
    "\n",
    "\n",
    "# 🔥 Run Analysis on Train & Test Loaders\n",
    "train_loader, test_loader = create_dataloaders(X_train, Y_train, X_test, Y_test, batch_size=32)\n",
    "\n",
    "analyze_dataloader(train_loader, dataset_name=\"Train\")\n",
    "analyze_dataloader(test_loader, dataset_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "def analyze_dataset(X_train, X_test, Y_train, Y_test, categorical_indices, numerical_indices):\n",
    "    \"\"\"\n",
    "    Analyze training and test dataset properties, including Y.\n",
    "    \n",
    "    - X_train, X_test: List of at-bat sequences, each a list of pitch feature vectors.\n",
    "    - Y_train, Y_test: List of at-bat sequences, each a dict with keys {'type', 'cont', 'result_desc', 'result_event'}.\n",
    "    - categorical_indices: List of indices for categorical features in X.\n",
    "    - numerical_indices: List of indices for numerical features in X.\n",
    "    \"\"\"\n",
    "    print(\"\\n🚀 **DATASET ANALYSIS** 🚀\\n\")\n",
    "\n",
    "    # Dataset sizes\n",
    "    print(f\"🔹 Training set: {len(X_train)} at-bats ({sum(len(x) for x in X_train)} pitches)\")\n",
    "    print(f\"🔹 Test set: {len(X_test)} at-bats ({sum(len(x) for x in X_test)} pitches)\")\n",
    "    print(f\"🔹 Y_train size: {len(Y_train)} at-bats\")\n",
    "    print(f\"🔹 Y_test size: {len(Y_test)} at-bats\")\n",
    "\n",
    "    # **Check if X and Y have the same at-bat counts**\n",
    "    assert len(X_train) == len(Y_train), \"❌ Mismatch between X_train and Y_train sizes!\"\n",
    "    assert len(X_test) == len(Y_test), \"❌ Mismatch between X_test and Y_test sizes!\"\n",
    "    print(\"✅ X and Y sizes are consistent.\")\n",
    "\n",
    "    # **Sequence Lengths (Distribution)**\n",
    "    train_lengths = [len(seq) for seq in X_train]\n",
    "    test_lengths = [len(seq) for seq in X_test]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(train_lengths, bins=range(min(train_lengths), max(train_lengths) + 1), alpha=0.6, label=\"Train\", edgecolor='black')\n",
    "    plt.hist(test_lengths, bins=range(min(test_lengths), max(test_lengths) + 1), alpha=0.6, label=\"Test\", edgecolor='black')\n",
    "    plt.xlabel(\"At-Bat Sequence Length (# pitches)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"📊 Sequence Length Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"📏 Train sequence lengths: Min={min(train_lengths)}, Max={max(train_lengths)}, Mean={np.mean(train_lengths):.2f}\")\n",
    "    print(f\"📏 Test sequence lengths: Min={min(test_lengths)}, Max={max(test_lengths)}, Mean={np.mean(test_lengths):.2f}\\n\")\n",
    "\n",
    "    # **Categorical Feature Analysis in X**\n",
    "    cat_counts = {idx: collections.Counter() for idx in categorical_indices}\n",
    "    \n",
    "    for x_seq in X_train:\n",
    "        for pitch in x_seq:\n",
    "            for idx in categorical_indices:\n",
    "                cat_counts[idx][pitch[idx]] += 1  # Count occurrences of each value\n",
    "\n",
    "    print(\"🔢 **Categorical Feature Distributions (Train Set - X)**\")\n",
    "    for idx, counter in cat_counts.items():\n",
    "        print(f\"\\nFeature Index {idx}: {len(counter)} unique values\")\n",
    "        for key, count in counter.most_common(10):  # Show top 10 most frequent\n",
    "            print(f\"   {key}: {count}\")\n",
    "        if len(counter) > 10:\n",
    "            print(\"   ... (truncated)\")\n",
    "\n",
    "    # **Numerical Feature Statistics in X**\n",
    "    num_values = {idx: [] for idx in numerical_indices}\n",
    "\n",
    "    for x_seq in X_train:\n",
    "        for pitch in x_seq:\n",
    "            for idx in numerical_indices:\n",
    "                num_values[idx].append(pitch[idx])\n",
    "\n",
    "    print(\"\\n📉 **Numerical Feature Statistics (Train Set - X)**\")\n",
    "    for idx, values in num_values.items():\n",
    "        values = np.array(values)\n",
    "        print(f\"\\nFeature Index {idx}: Min={values.min():.2f}, Max={values.max():.2f}, Mean={values.mean():.2f}, Std={values.std():.2f}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # **Analyze Y Sequences**\n",
    "    # --------------------------\n",
    "\n",
    "    # **Check Distribution of Pitch Types in Y**\n",
    "    type_counts = collections.Counter()\n",
    "    desc_counts = collections.Counter()\n",
    "    event_counts = collections.Counter()\n",
    "    cont_values = {i: [] for i in range(5)}  # Stores numerical `cont` values\n",
    "\n",
    "    for y_seq in Y_train:\n",
    "        for y_t in y_seq:\n",
    "            type_counts[y_t['type']] += 1\n",
    "            desc_counts[y_t['result_desc']] += 1\n",
    "            event_counts[y_t['result_event']] += 1\n",
    "            for i, val in enumerate(y_t['cont']):\n",
    "                cont_values[i].append(val)\n",
    "\n",
    "    # **Pitch Type Distribution**\n",
    "    print(\"\\n🎯 **Pitch Type Distribution (Train Set - Y)**\")\n",
    "    for pitch_type, count in type_counts.most_common(10):\n",
    "        print(f\"   {pitch_type}: {count}\")\n",
    "    if len(type_counts) > 10:\n",
    "        print(\"   ... (truncated)\")\n",
    "\n",
    "    # **Pitch Result Description Distribution**\n",
    "    print(\"\\n📝 **Pitch Result Description (Train Set - Y)**\")\n",
    "    for desc, count in desc_counts.most_common(10):\n",
    "        print(f\"   {desc}: {count}\")\n",
    "    if len(desc_counts) > 10:\n",
    "        print(\"   ... (truncated)\")\n",
    "\n",
    "    # **Pitch Result Event Distribution**\n",
    "    print(\"\\n⚾ **Pitch Result Event (Train Set - Y)**\")\n",
    "    for event, count in event_counts.most_common(10):\n",
    "        print(f\"   {event}: {count}\")\n",
    "    if len(event_counts) > 10:\n",
    "        print(\"   ... (truncated)\")\n",
    "\n",
    "    # **Numerical Feature Statistics in Y (cont values)**\n",
    "    print(\"\\n📊 **Numerical Features in Y_cont (Train Set)**\")\n",
    "    for idx, values in cont_values.items():\n",
    "        values = np.array(values)\n",
    "        print(f\"\\nFeature {idx}: Min={values.min():.2f}, Max={values.max():.2f}, Mean={values.mean():.2f}, Std={values.std():.2f}\")\n",
    "\n",
    "    print(\"\\n✅ **Full Dataset Analysis Complete!** ✅\\n\")\n",
    "\n",
    "\n",
    "analyze_dataset(X_train, X_test, Y_train, Y_test, cat_indices, num_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['pitcher'].unique()))\n",
    "print(len(df['batter'].unique()))\n",
    "print(len(df['pitch_type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total At-Bat Sequences: 1010397\n",
      "Example sequence lengths:\n",
      "At-bat 0: 2 pitches\n",
      "At-bat 1: 5 pitches\n",
      "At-bat 2: 4 pitches\n",
      "At-bat 3: 2 pitches\n",
      "At-bat 4: 1 pitches\n",
      "Total At-Bat Sequences: 1010397\n",
      "Example sequence lengths:\n",
      "At-bat 0: 2 pitches\n",
      "At-bat 1: 5 pitches\n",
      "At-bat 2: 4 pitches\n",
      "At-bat 3: 2 pitches\n",
      "At-bat 4: 1 pitches\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total At-Bat Sequences: {len(X_sequences)}\")\n",
    "\n",
    "# Check the length of a few sequences\n",
    "print(\"Example sequence lengths:\")\n",
    "for i in range(5):  # Check first 5 at-bats\n",
    "    print(f\"At-bat {i}: {len(X_sequences[i])} pitches\")\n",
    "\n",
    "\n",
    "print(f\"Total At-Bat Sequences: {len(Y_sequences)}\")\n",
    "\n",
    "# Check the length of a few sequences\n",
    "print(\"Example sequence lengths:\")\n",
    "for i in range(5):  # Check first 5 at-bats\n",
    "    print(f\"At-bat {i}: {len(Y_sequences[i])} pitches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AT_BATS = 3  #Number of at-bats to check\n",
    "for ab_idx in range(NUM_AT_BATS):\n",
    "    print(f\"\\nAt-Bat {ab_idx+1} (Total Pitches: {len(X_sequences[ab_idx])})\")\n",
    "    for i, pitch in enumerate(X_sequences[ab_idx]):\n",
    "        print(f\"  Pitch {i+1}: {pitch}\")\n",
    "        print(f\"  Feature Count: {len(pitch)}\\n\")\n",
    "\n",
    "\n",
    "NUM_AT_BATS = 3  #Number of at-bats to check\n",
    "for ab_idx in range(NUM_AT_BATS):\n",
    "    print(f\"\\nAt-Bat {ab_idx+1} (Total Pitches: {len(Y_sequences[ab_idx])})\")\n",
    "    for i, pitch in enumerate(Y_sequences[ab_idx]):\n",
    "        print(f\"  Pitch {i+1}: {pitch}\")\n",
    "        print(f\"  Feature Count: {len(pitch)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feature_outliers(X_sequences):\n",
    "    \"\"\"\n",
    "    Checks how many values in X_sequences fall outside of the suggested valid ranges\n",
    "    for release speed, release spin rate, plate_x, and plate_z.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define valid ranges for each feature\n",
    "    VALID_RANGES = {\n",
    "        \"release_speed\": (50, 110),      # MPH\n",
    "        \"release_spin_rate\": (500, 3700),  # RPM\n",
    "        \"plate_x\": (-3, 3),              # Feet\n",
    "        \"plate_z\": (-1, 4)                 # Feet\n",
    "    }\n",
    "\n",
    "    # Indices of the features in the X_sequences\n",
    "    FEATURE_INDICES = {\n",
    "        \"release_speed\": 16,\n",
    "        \"release_spin_rate\": 17,\n",
    "        \"plate_x\": 19,\n",
    "        \"plate_z\": 20\n",
    "    }\n",
    "\n",
    "    # Initialize counters for out-of-range values\n",
    "    outlier_counts = {feature: 0 for feature in VALID_RANGES.keys()}\n",
    "    total_counts = {feature: 0 for feature in VALID_RANGES.keys()}\n",
    "\n",
    "    # Iterate over each pitch in each at-bat sequence\n",
    "    for at_bat in X_sequences:\n",
    "        for pitch in at_bat:\n",
    "            for feature, (lower, upper) in VALID_RANGES.items():\n",
    "                idx = FEATURE_INDICES[feature]\n",
    "                value = pitch[idx]\n",
    "\n",
    "                if isinstance(value, (int, float)):  # Ensure it's numeric\n",
    "                    total_counts[feature] += 1\n",
    "                    if value < lower or value > upper:\n",
    "                        outlier_counts[feature] += 1\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nOutlier Analysis for Selected Numeric Features:\")\n",
    "    for feature in VALID_RANGES.keys():\n",
    "        total = total_counts[feature]\n",
    "        outliers = outlier_counts[feature]\n",
    "        print(f\"  {feature}: {outliers}/{total} ({(outliers / total) * 100:.2f}% outliers)\")\n",
    "\n",
    "# Run the check on X_sequences\n",
    "check_feature_outliers(X_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, at_bat in enumerate(X_sequences[:3]):  # Check first 3 at-bats\n",
    "    print(f\"\\nAt-Bat {i+1} (Total Pitches: {len(at_bat)})\")\n",
    "    for j, pitch in enumerate(at_bat):\n",
    "        extracted_numeric = [pitch[idx] for idx in NUMERIC_INDICES]\n",
    "        print(f\"  Pitch {j+1} Numeric Features: {extracted_numeric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Categorical Value Mappings (Limited View) ===\n",
      "\n",
      "Column 0 - Total Unique Categories: 5\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "   2 -> 2\n",
      "   3 -> 3\n",
      "   4 -> 4\n",
      "\n",
      "Column 1 - Total Unique Categories: 4\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "   2 -> 2\n",
      "   3 -> 3\n",
      "\n",
      "Column 2 - Total Unique Categories: 3\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "   2 -> 2\n",
      "\n",
      "Column 3 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 4 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 5 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 6 - Total Unique Categories: 19\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "   2 -> 2\n",
      "   3 -> 3\n",
      "   4 -> 4\n",
      "   5 -> 5\n",
      "   6 -> 6\n",
      "   7 -> 7\n",
      "   8 -> 8\n",
      "   9 -> 9\n",
      "   10 -> 10\n",
      "   11 -> 11\n",
      "   12 -> 12\n",
      "   13 -> 13\n",
      "   14 -> 14\n",
      "   ... (truncated)\n",
      "\n",
      "Column 7 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 10 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 11 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 12 - Total Unique Categories: 878\n",
      "   424144 -> 0\n",
      "   425794 -> 1\n",
      "   425844 -> 2\n",
      "   434378 -> 3\n",
      "   445213 -> 4\n",
      "   445276 -> 5\n",
      "   445926 -> 6\n",
      "   446321 -> 7\n",
      "   446372 -> 8\n",
      "   448179 -> 9\n",
      "   448281 -> 10\n",
      "   450203 -> 11\n",
      "   453178 -> 12\n",
      "   453265 -> 13\n",
      "   453268 -> 14\n",
      "   ... (truncated)\n",
      "\n",
      "Column 13 - Total Unique Categories: 3095\n",
      "   112526 -> 0\n",
      "   116338 -> 1\n",
      "   120074 -> 2\n",
      "   121347 -> 3\n",
      "   133380 -> 4\n",
      "   134181 -> 5\n",
      "   136860 -> 6\n",
      "   150029 -> 7\n",
      "   150212 -> 8\n",
      "   150229 -> 9\n",
      "   150302 -> 10\n",
      "   150359 -> 11\n",
      "   217100 -> 12\n",
      "   218596 -> 13\n",
      "   276519 -> 14\n",
      "   ... (truncated)\n",
      "\n",
      "Column 14 - Total Unique Categories: 2\n",
      "   0 -> 0\n",
      "   1 -> 1\n",
      "\n",
      "Column 15 - Total Unique Categories: 15\n",
      "   CH -> 0\n",
      "   CS -> 1\n",
      "   CU -> 2\n",
      "   EP -> 3\n",
      "   FA -> 4\n",
      "   FC -> 5\n",
      "   FF -> 6\n",
      "   FS -> 7\n",
      "   KC -> 8\n",
      "   KN -> 9\n",
      "   NONE -> 10\n",
      "   SI -> 11\n",
      "   SL -> 12\n",
      "   ST -> 13\n",
      "   SV -> 14\n",
      "\n",
      "Column 21 - Total Unique Categories: 2\n",
      "   IN_PROGRESS -> 0\n",
      "   NONE -> 1\n",
      "\n",
      "Column 22 - Total Unique Categories: 11\n",
      "   NONE -> 0\n",
      "   ball -> 1\n",
      "   blocked_ball -> 2\n",
      "   bunt_foul_tip -> 3\n",
      "   called_strike -> 4\n",
      "   foul -> 5\n",
      "   foul_bunt -> 6\n",
      "   foul_tip -> 7\n",
      "   missed_bunt -> 8\n",
      "   swinging_strike -> 9\n",
      "   swinging_strike_blocked -> 10\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Set a limit for the number of values to print per column\n",
    "max_display = 15  # Change this if you want more/less displayed\n",
    "\n",
    "print(\"\\n=== Categorical Value Mappings (Limited View) ===\")\n",
    "for col_idx in cat_indices:\n",
    "    le = label_encoders_X[col_idx]\n",
    "    mapping_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    \n",
    "    # Print total unique values in this categorical column\n",
    "    print(f\"\\nColumn {col_idx} - Total Unique Categories: {len(mapping_dict)}\")\n",
    "\n",
    "    # Print only a limited number of mappings\n",
    "    for i, (original, encoded) in enumerate(mapping_dict.items()):\n",
    "        if i >= max_display:\n",
    "            print(\"   ... (truncated)\")\n",
    "            break\n",
    "        print(f\"   {original} -> {encoded}\")\n",
    "    \n",
    "    # Optional: Store full mapping for later debugging (don't print it)\n",
    "    full_mappings = mapping_dict\n",
    "\n",
    "print(\"\\n===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Unique Categorical Values Before Encoding ===\n",
      "\n",
      "Column 0 - Total Unique Categories: 5\n",
      "   0\n",
      "   1\n",
      "   2\n",
      "   3\n",
      "   4\n",
      "\n",
      "Column 1 - Total Unique Categories: 4\n",
      "   0\n",
      "   1\n",
      "   2\n",
      "   3\n",
      "\n",
      "Column 2 - Total Unique Categories: 3\n",
      "   0\n",
      "   1\n",
      "   2\n",
      "\n",
      "Column 3 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 4 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 5 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 6 - Total Unique Categories: 19\n",
      "   0\n",
      "   1\n",
      "   2\n",
      "   3\n",
      "   4\n",
      "   5\n",
      "   6\n",
      "   7\n",
      "   8\n",
      "   9\n",
      "   10\n",
      "   11\n",
      "   12\n",
      "   13\n",
      "   14\n",
      "   ... (truncated)\n",
      "\n",
      "Column 7 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 10 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 11 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 12 - Total Unique Categories: 878\n",
      "   424144\n",
      "   425794\n",
      "   425844\n",
      "   434378\n",
      "   445213\n",
      "   445276\n",
      "   445926\n",
      "   446321\n",
      "   446372\n",
      "   448179\n",
      "   448281\n",
      "   450203\n",
      "   453178\n",
      "   453265\n",
      "   453268\n",
      "   ... (truncated)\n",
      "\n",
      "Column 13 - Total Unique Categories: 3095\n",
      "   112526\n",
      "   116338\n",
      "   120074\n",
      "   121347\n",
      "   133380\n",
      "   134181\n",
      "   136860\n",
      "   150029\n",
      "   150212\n",
      "   150229\n",
      "   150302\n",
      "   150359\n",
      "   217100\n",
      "   218596\n",
      "   276519\n",
      "   ... (truncated)\n",
      "\n",
      "Column 14 - Total Unique Categories: 2\n",
      "   0\n",
      "   1\n",
      "\n",
      "Column 15 - Total Unique Categories: 15\n",
      "   CH\n",
      "   CS\n",
      "   CU\n",
      "   EP\n",
      "   FA\n",
      "   FC\n",
      "   FF\n",
      "   FS\n",
      "   KC\n",
      "   KN\n",
      "   NONE\n",
      "   SI\n",
      "   SL\n",
      "   ST\n",
      "   SV\n",
      "\n",
      "Column 21 - Total Unique Categories: 2\n",
      "   IN_PROGRESS\n",
      "   NONE\n",
      "\n",
      "Column 22 - Total Unique Categories: 11\n",
      "   NONE\n",
      "   ball\n",
      "   blocked_ball\n",
      "   bunt_foul_tip\n",
      "   called_strike\n",
      "   foul\n",
      "   foul_bunt\n",
      "   foul_tip\n",
      "   missed_bunt\n",
      "   swinging_strike\n",
      "   swinging_strike_blocked\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "unique_cats = {i: set() for i in cat_indices}\n",
    "\n",
    "    # Collect unique values from X_sequences\n",
    "for at_bat in X_sequences:\n",
    "    for pitch in at_bat:\n",
    "        for i in cat_indices:\n",
    "            unique_cats[i].add(pitch[i])\n",
    "\n",
    "    # Print results with a limited display\n",
    "print(\"\\n=== Unique Categorical Values Before Encoding ===\")\n",
    "for col_idx, values in unique_cats.items():\n",
    "    print(f\"\\nColumn {col_idx} - Total Unique Categories: {len(values)}\")\n",
    "    for j, val in enumerate(sorted(values)):  # Sort for better readability\n",
    "        if j >= max_display:\n",
    "            print(\"   ... (truncated)\")\n",
    "            break\n",
    "        print(f\"   {val}\")\n",
    "\n",
    "print(\"\\n===================================\")\n",
    "    \n",
    "#print(unique_cats)  # Return the dictionary if you want to inspect further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def verify_processed_data(X_sequences, processed_X, Y_sequences, processed_Y, num_indices, cat_indices):\n",
    "    \"\"\"\n",
    "    Verifies that processed X and Y sequences retain their original shape, \n",
    "    categorical encoding is correct, and numerical scaling is valid.\n",
    "\n",
    "    Parameters:\n",
    "        X_sequences (list): Original nested list of pitch sequences.\n",
    "        processed_X (list): Transformed X sequences with encoded and scaled features.\n",
    "        Y_sequences (list): Original nested list of dictionaries for Y.\n",
    "        processed_Y (list): Processed Y sequences, a list of at-bat sequences.\n",
    "        num_indices (list): Indices of numerical features in X.\n",
    "        cat_indices (list): Indices of categorical features in X.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== SHAPE VERIFICATION ===\")\n",
    "    \n",
    "    # Check at-bat counts\n",
    "    assert len(X_sequences) == len(processed_X), \"Mismatch in at-bat count for X\"\n",
    "    assert len(Y_sequences) == len(processed_Y), \"Mismatch in at-bat count for Y\"\n",
    "\n",
    "    # Check pitch sequence lengths\n",
    "    for i, (orig_x, proc_x, orig_y, proc_y) in enumerate(zip(X_sequences, processed_X, Y_sequences, processed_Y)):\n",
    "        assert len(orig_x) == len(proc_x), f\"Mismatch in pitch count in X at-bat {i}\"\n",
    "        assert len(orig_y) == len(proc_y), f\"Mismatch in pitch count in Y at-bat {i}\"\n",
    "        assert len(orig_x[0]) == len(proc_x[0]), f\"Mismatch in feature count in X at-bat {i}\"\n",
    "\n",
    "    print(\"✅ X and Y shape verified.\")\n",
    "\n",
    "    print(\"\\n=== CATEGORICAL ENCODING VERIFICATION ===\")\n",
    "    \n",
    "    # Store original and processed categorical values\n",
    "    original_cats = {i: set() for i in cat_indices}\n",
    "    encoded_cats = {i: set() for i in cat_indices}\n",
    "\n",
    "    # Collect original categorical values\n",
    "    for at_bat in X_sequences:\n",
    "        for pitch in at_bat:\n",
    "            for i in cat_indices:\n",
    "                original_cats[i].add(pitch[i])\n",
    "\n",
    "    # Collect encoded categorical values\n",
    "    for at_bat in processed_X:\n",
    "        for pitch in at_bat:\n",
    "            for i in cat_indices:\n",
    "                encoded_cats[i].add(pitch[i])\n",
    "\n",
    "    # Compare original vs encoded\n",
    "    for i in cat_indices:\n",
    "        assert len(original_cats[i]) == len(encoded_cats[i]), f\"Mismatch in categorical encoding for feature index {i}\"\n",
    "    \n",
    "    print(\"✅ Categorical encoding verified.\")\n",
    "\n",
    "    print(\"\\n=== NUMERICAL SCALING VERIFICATION ===\")\n",
    "\n",
    "    # Extract numerical features from processed_X\n",
    "    processed_num_features = np.array([\n",
    "        [pitch[i] for i in num_indices] for at_bat in processed_X for pitch in at_bat\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    mean_values = np.mean(processed_num_features, axis=0)\n",
    "    std_values = np.std(processed_num_features, axis=0)\n",
    "\n",
    "    print(f\"Mean values (should be ~0): {mean_values}\")\n",
    "    print(f\"Standard deviations (should be ~1): {std_values}\")\n",
    "\n",
    "    assert np.allclose(mean_values, 0, atol=0.1), \"Mean is not close to 0, scaling issue\"\n",
    "    assert np.allclose(std_values, 1, atol=0.1), \"Standard deviation is not close to 1, scaling issue\"\n",
    "    print(\"✅ Numerical scaling verified.\")\n",
    "\n",
    "    print(\"\\n=== Y_cont SCALING VERIFICATION ===\")\n",
    "\n",
    "    # Extract 'cont' values properly\n",
    "    Y_cont_array = np.array([y_t['cont'] for seq in processed_Y for y_t in seq], dtype=np.float64)\n",
    "\n",
    "    mean_Y_cont = np.mean(Y_cont_array, axis=0)\n",
    "    std_Y_cont = np.std(Y_cont_array, axis=0)\n",
    "\n",
    "    print(f\"Y_cont mean values (should be ~0): {mean_Y_cont}\")\n",
    "    print(f\"Y_cont standard deviations (should be ~1): {std_Y_cont}\")\n",
    "\n",
    "    assert np.allclose(mean_Y_cont, 0, atol=0.1), \"Y_cont mean is not close to 0\"\n",
    "    assert np.allclose(std_Y_cont, 1, atol=0.1), \"Y_cont standard deviation is not close to 1\"\n",
    "    print(\"✅ Y_cont scaling verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def verify_processed_dataasd(X_sequences, processed_X, Y_sequences, processed_Y, num_indices, cat_indices):\n",
    "    \"\"\"\n",
    "    Verifies that processed X and Y sequences retain their original shape, \n",
    "    categorical encoding is correct, and numerical scaling is valid.\n",
    "\n",
    "    Parameters:\n",
    "        X_sequences (list): Original nested list of pitch sequences.\n",
    "        processed_X (list): Transformed X sequences with encoded and scaled features.\n",
    "        Y_sequences (list): Original nested list of dictionaries for Y.\n",
    "        processed_Y (dict): Processed dictionary containing encoded/scaled Y components.\n",
    "        num_indices (list): Indices of numerical features in X.\n",
    "        cat_indices (list): Indices of categorical features in X.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== SHAPE VERIFICATION ===\")\n",
    "    assert len(X_sequences) == len(processed_X), \"Mismatch in at-bat count\"\n",
    "    for i, (orig, proc) in enumerate(zip(X_sequences, processed_X)):\n",
    "        assert len(orig) == len(proc), f\"Mismatch in pitch count in at-bat {i}\"\n",
    "        assert len(orig[0]) == len(proc[0]), f\"Mismatch in feature count in at-bat {i}\"\n",
    "    print(\"✅ X shape verified.\")\n",
    "\n",
    "    assert len(Y_sequences) == len(processed_Y['type']), \"Mismatch in Y at-bat count\"\n",
    "    for i, (orig, proc_type, proc_cont, proc_desc, proc_event) in enumerate(zip(Y_sequences, processed_Y['type'], processed_Y['cont'], processed_Y['result_desc'], processed_Y['result_event'])):\n",
    "        assert len(orig) == len(proc_type) == len(proc_cont) == len(proc_desc) == len(proc_event), f\"Mismatch in pitch count in Y at-bat {i}\"\n",
    "    print(\"✅ Y shape verified.\")\n",
    "\n",
    "    print(\"\\n=== CATEGORICAL ENCODING VERIFICATION ===\")\n",
    "    original_cats = set()\n",
    "    encoded_cats = set()\n",
    "\n",
    "    for at_bat in X_sequences:\n",
    "        for pitch in at_bat:\n",
    "            for i in cat_indices:\n",
    "                original_cats.add(pitch[i])\n",
    "\n",
    "    for at_bat in processed_X:\n",
    "        for pitch in at_bat:\n",
    "            for i in range(len(pitch)):  # Encoded categorical values should be integers\n",
    "                if i in cat_indices:\n",
    "                    encoded_cats.add(pitch[i])\n",
    "\n",
    "    print(f\"Original categorical values: {original_cats}\")\n",
    "    print(f\"Encoded categorical values: {encoded_cats}\")\n",
    "\n",
    "    print(len(original_cats) == len(encoded_cats)), \"Mismatch in categorical encoding\"\n",
    "    print(\"✅ Categorical encoding verified.\")\n",
    "\n",
    "    print(\"\\n=== NUMERICAL SCALING VERIFICATION ===\")\n",
    "    processed_num_features = []\n",
    "\n",
    "    for at_bat in processed_X:\n",
    "        for pitch in at_bat:\n",
    "            processed_num_features.append([pitch[i] for i in num_indices])\n",
    "\n",
    "    processed_num_features = np.array(processed_num_features)\n",
    "\n",
    "    mean_values = np.mean(processed_num_features, axis=0)\n",
    "    std_values = np.std(processed_num_features, axis=0)\n",
    "\n",
    "    print(f\"Mean values (should be ~0): {mean_values}\")\n",
    "    print(f\"Standard deviations (should be ~1): {std_values}\")\n",
    "\n",
    "    assert np.allclose(mean_values, 0, atol=0.1), \"Mean is not close to 0, scaling issue\"\n",
    "    assert np.allclose(std_values, 1, atol=0.1), \"Standard deviation is not close to 1, scaling issue\"\n",
    "    print(\"✅ Numerical scaling verified.\")\n",
    "\n",
    "    print(\"\\n=== Y_cont SCALING VERIFICATION ===\")\n",
    "    Y_cont_array = np.array([cont for seq in processed_Y['cont'] for cont in seq])\n",
    "\n",
    "    mean_Y_cont = np.mean(Y_cont_array, axis=0)\n",
    "    std_Y_cont = np.std(Y_cont_array, axis=0)\n",
    "\n",
    "    print(f\"Y_cont mean values (should be ~0): {mean_Y_cont}\")\n",
    "    print(f\"Y_cont standard deviations (should be ~1): {std_Y_cont}\")\n",
    "\n",
    "    assert np.allclose(mean_Y_cont, 0, atol=0.1), \"Y_cont mean is not close to 0\"\n",
    "    assert np.allclose(std_Y_cont, 1, atol=0.1), \"Y_cont standard deviation is not close to 1\"\n",
    "    print(\"✅ Y_cont scaling verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE VERIFICATION ===\n",
      "✅ X and Y shape verified.\n",
      "\n",
      "=== CATEGORICAL ENCODING VERIFICATION ===\n",
      "✅ Categorical encoding verified.\n",
      "\n",
      "=== NUMERICAL SCALING VERIFICATION ===\n",
      "Mean values (should be ~0): [-1.54397806e-13 -9.14623623e-15 -2.24886452e-13 -6.75048608e-14\n",
      " -2.40669974e-12 -1.03868605e-13 -9.51524570e-14]\n",
      "Standard deviations (should be ~1): [1. 1. 1. 1. 1. 1. 1.]\n",
      "✅ Numerical scaling verified.\n",
      "\n",
      "=== Y_cont SCALING VERIFICATION ===\n",
      "Y_cont mean values (should be ~0): [ 9.21193614e-13  2.32952204e-15 -8.80339385e-13 -1.64679682e-14\n",
      " -3.33577595e-13]\n",
      "Y_cont standard deviations (should be ~1): [1. 1. 1. 1. 1.]\n",
      "✅ Y_cont scaling verified.\n"
     ]
    }
   ],
   "source": [
    "verify_processed_data(X_sequences, processed_X, Y_sequences, processed_Y, num_indices, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_processed_sequences(processed_X, processed_Y, max_at_bats=2):\n",
    "    \"\"\"\n",
    "    Formats the processed sequences into a readable structure like the original format.\n",
    "    \n",
    "    Parameters:\n",
    "        processed_X (list): Encoded and scaled X sequences.\n",
    "        processed_Y (dict): Encoded and scaled Y sequences.\n",
    "        max_at_bats (int, optional): Maximum number of at-bats to display. Default is 2.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted string representation of the processed sequences.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    for at_bat_idx, (x_at_bat, y_type, y_cont, y_desc, y_event) in enumerate(\n",
    "            zip(processed_X, processed_Y['type'], processed_Y['cont'], processed_Y['result_desc'], processed_Y['result_event'])):\n",
    "        \n",
    "        if at_bat_idx >= max_at_bats:\n",
    "            break  # Stop after reaching max_at_bats\n",
    "\n",
    "        output.append(f\"At-Bat {at_bat_idx + 1} (Total Pitches: {len(x_at_bat)})\")\n",
    "        \n",
    "        for pitch_idx, (x_pitch, y_type_pitch, y_cont_pitch, y_desc_pitch, y_event_pitch) in enumerate(\n",
    "                zip(x_at_bat, y_type, y_cont, y_desc, y_event)):\n",
    "            \n",
    "            # Convert lists to numpy-friendly format\n",
    "            formatted_x_pitch = [\n",
    "                f\"np.int64({int(val)})\" if isinstance(val, (int, np.integer)) else\n",
    "                f\"np.float64({float(val):.2f})\" if isinstance(val, (float, np.floating)) else val\n",
    "                for val in x_pitch\n",
    "            ]\n",
    "            \n",
    "            # Convert Y_cont (5 numeric features) to formatted np.float64 values\n",
    "            formatted_y_cont = [f\"np.float64({float(val):.2f})\" for val in y_cont_pitch]\n",
    "            \n",
    "            # Format full Y pitch data\n",
    "            formatted_y = f\"Y -> Type: {y_type_pitch}, Cont: {formatted_y_cont}, Desc: {y_desc_pitch}, Event: {y_event_pitch}\"\n",
    "            \n",
    "            output.append(f\"  Pitch {pitch_idx + 1}: {formatted_x_pitch}\")\n",
    "            output.append(f\"  Feature Count: {len(x_pitch)}\")\n",
    "            output.append(f\"  {formatted_y}\")\n",
    "            output.append(\"\")  # Blank line for readability\n",
    "    \n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_output = format_processed_sequences(processed_X, processed_Y, max_at_bats=4)\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def extract_features_from_sequences(processed_X, processed_Y, max_at_bats=None):\n",
    "    \"\"\"\n",
    "    Converts processed X and Y sequences into a structured Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        processed_X (list): Encoded and scaled X sequences.\n",
    "        processed_Y (dict): Encoded and scaled Y sequences.\n",
    "        max_at_bats (int, optional): Maximum number of at-bats to include in the DataFrame. Default is None (include all).\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): DataFrame containing structured pitch features.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for at_bat_idx, (x_at_bat, y_type, y_cont, y_desc, y_event) in enumerate(\n",
    "            zip(processed_X, processed_Y['type'], processed_Y['cont'], processed_Y['result_desc'], processed_Y['result_event'])):\n",
    "        \n",
    "        # Stop if max_at_bats is reached\n",
    "        if max_at_bats is not None and at_bat_idx >= max_at_bats:\n",
    "            break\n",
    "        \n",
    "        for pitch_idx, (x_pitch, y_type_pitch, y_cont_pitch, y_desc_pitch, y_event_pitch) in enumerate(\n",
    "                zip(x_at_bat, y_type, y_cont, y_desc, y_event)):\n",
    "            \n",
    "            # Flatten X pitch features\n",
    "            pitch_data = [at_bat_idx + 1, pitch_idx + 1] + x_pitch\n",
    "            \n",
    "            # Append Y values separately\n",
    "            pitch_data += [y_type_pitch] + list(y_cont_pitch) + [y_desc_pitch, y_event_pitch]\n",
    "            \n",
    "            # Store data\n",
    "            data.append(pitch_data)\n",
    "\n",
    "    # Create column names dynamically\n",
    "    num_x_features = len(processed_X[0][0])  # Number of features in X\n",
    "    num_y_cont_features = len(processed_Y['cont'][0][0])  # Number of continuous Y features\n",
    "\n",
    "    column_names = ['At-Bat', 'Pitch'] + [f'X_{i}' for i in range(num_x_features)] + \\\n",
    "                   ['Y_Type'] + [f'Y_Cont_{i}' for i in range(num_y_cont_features)] + ['Y_Desc', 'Y_Event']\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df_features = extract_features_from_sequences(processed_X, processed_Y, max_at_bats=None)\n",
    "\n",
    "# Display first few rows\n",
    "display(df_features)\n",
    "\n",
    "# Save to CSV if needed\n",
    "# df_features.to_csv(\"pitch_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(df_features['Y_Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Unique Categorical Values in Y Before Encoding ===\n",
      "\n",
      "Y_Type - Total Unique Categories: 15\n",
      "   AB\n",
      "   CH\n",
      "   CS\n",
      "   CU\n",
      "   EP\n",
      "   FA\n",
      "   FC\n",
      "   FF\n",
      "   FS\n",
      "   KC\n",
      "   ... (truncated)\n",
      "\n",
      "Y_Desc - Total Unique Categories: 13\n",
      "   ball\n",
      "   blocked_ball\n",
      "   bunt_foul_tip\n",
      "   called_strike\n",
      "   foul\n",
      "   foul_bunt\n",
      "   foul_tip\n",
      "   hit_by_pitch\n",
      "   hit_into_play\n",
      "   intent_ball\n",
      "   ... (truncated)\n",
      "\n",
      "Y_Event - Total Unique Categories: 23\n",
      "   IN_PROGRESS\n",
      "   catcher_interf\n",
      "   double\n",
      "   double_play\n",
      "   field_error\n",
      "   field_out\n",
      "   fielders_choice\n",
      "   fielders_choice_out\n",
      "   force_out\n",
      "   grounded_into_double_play\n",
      "   ... (truncated)\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "def get_unique_values_from_Y(Y_sequences, max_display=10):\n",
    "    \"\"\"\n",
    "    Extracts and prints unique values for 'type', 'desc', and 'event' from Y_sequences before encoding.\n",
    "\n",
    "    Parameters:\n",
    "        Y_sequences (list): Original Y sequences containing 'type', 'desc', and 'event'.\n",
    "        max_display (int, optional): Limits how many unique values are displayed per category.\n",
    "    \n",
    "    Returns:\n",
    "        unique_values (dict): Dictionary of unique values per category.\n",
    "    \"\"\"\n",
    "    unique_values = {\n",
    "        \"Y_Type\": set(),\n",
    "        \"Y_Desc\": set(),\n",
    "        \"Y_Event\": set()\n",
    "    }\n",
    "\n",
    "    # Collect unique values\n",
    "    for at_bat in Y_sequences:\n",
    "        for pitch in at_bat:\n",
    "            unique_values[\"Y_Type\"].add(pitch[\"type\"])\n",
    "            unique_values[\"Y_Desc\"].add(pitch[\"result\"][0])  # First element of 'result' is desc\n",
    "            unique_values[\"Y_Event\"].add(pitch[\"result\"][1])  # Second element of 'result' is event\n",
    "\n",
    "    # Print results with a limited display\n",
    "    print(\"\\n=== Unique Categorical Values in Y Before Encoding ===\")\n",
    "    for category, values in unique_values.items():\n",
    "        print(f\"\\n{category} - Total Unique Categories: {len(values)}\")\n",
    "        for j, val in enumerate(sorted(values)):  # Sort for better readability\n",
    "            if j >= max_display:\n",
    "                print(\"   ... (truncated)\")\n",
    "                break\n",
    "            print(f\"   {val}\")\n",
    "    \n",
    "    print(\"\\n===================================\")\n",
    "    \n",
    "    return unique_values  # Return dictionary for further use if needed\n",
    "\n",
    "# Example usage:\n",
    "unique_Y_values = get_unique_values_from_Y(Y_sequences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
